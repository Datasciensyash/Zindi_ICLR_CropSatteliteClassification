{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from modules.loss import Multiclass_focal_loss\n",
    "from modules.loss import Binary_focal_loss\n",
    "\n",
    "from models.scn import SCNet\n",
    "from modules.dataset import CropDataset\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 9032020\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model_class = lgb.LGBMClassifier\n",
    "settings = {\n",
    "    'CHANNELS': 64,\n",
    "    'NUM_CHANNELS': 6,\n",
    "    'INPUT_SIZE': 5,\n",
    "    'BANDS': 'B02 B03 B04 B08 B12 CLD'.split(' '),\n",
    "    'NUM_EPOCHS': 16,\n",
    "    'BATCH_SIZE': 128,\n",
    "    'ADAM_LR': 1e-4,\n",
    "    'DATA_FOLDER': 'dataframes/',\n",
    "    'DEVICE': torch.device('cuda:0')\n",
    "}\n",
    "boost_settings = {\n",
    "        \"n_estimators\": 128,\n",
    "        \"random_state\": seed,\n",
    "        \"objective\" : \"multiclass\",\n",
    "        \"num_class\" : 7,\n",
    "        \"class_weight\": None,\n",
    "        \"num_leaves\" : 60,\n",
    "        \"max_depth\": -1,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"subsample\" : 0.9,\n",
    "        \"colsample_bytree\" : 0.4,\n",
    "        \"verbose\" : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(class_num, CHANNELS, NUM_CHANNELS, INPUT_SIZE, BANDS, NUM_EPOCHS, BATCH_SIZE, ADAM_LR, DATA_FOLDER, DEVICE):\n",
    "    \n",
    "    #Get data\n",
    "    train_df = pd.read_pickle(DATA_FOLDER + 'train_data.csv')\n",
    "    test_df = pd.read_pickle(DATA_FOLDER + 'test_data.csv')\n",
    "    \n",
    "    #Get lambda-function for dataframe label change\n",
    "    DATAFRAME_LAMBDA = lambda a: 1 if a == class_num else 0\n",
    "        \n",
    "    #Create dataloder instances\n",
    "    train_dataset = CropDataset(train_df, bands=BANDS, crop_size=INPUT_SIZE, classf=DATAFRAME_LAMBDA, autobalance=class_num)\n",
    "    test_dataset = CropDataset(test_df, bands=BANDS, crop_size=INPUT_SIZE, classf=DATAFRAME_LAMBDA, need_id=True)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    #Create NN instance\n",
    "    Model = SCNet(input_size=INPUT_SIZE, num_inputs=NUM_CHANNELS, in_channels=1, out_channels=CHANNELS, num_classes=1).to(DEVICE)\n",
    "    \n",
    "    #Criterion and optimizer\n",
    "    fl = Binary_focal_loss(gamma=2.0, alpha=1.0)\n",
    "    optimizer = torch.optim.Adam(Model.parameters(), lr=ADAM_LR)\n",
    "    \n",
    "    #Train loop\n",
    "    for epoch_num in range(NUM_EPOCHS):\n",
    "        for X, Y in iter(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            X, Y = X.to(DEVICE), Y.to(DEVICE)\n",
    "            #MinMax Normalization for better training\n",
    "            X = (X - X.min()) / (X.max() - X.min())\n",
    "            #\n",
    "            Y_pred = Model(X)\n",
    "            loss = fl(Y.float(), Y_pred.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    torch.save(Model.state_dict(), f'Model--{class_num}.pt')\n",
    "    \n",
    "    Model.eval()\n",
    "    \n",
    "    #Predict test set\n",
    "    F_IDS = []\n",
    "    PREDS = []\n",
    "    for X, F_ID in iter(test_dataloader):\n",
    "        Y_pred = Model(X.to(DEVICE))\n",
    "        F_IDS.extend(list(F_ID.numpy()))\n",
    "        PREDS.extend(list(Y_pred.squeeze(1).detach().cpu().numpy()))\n",
    "    \n",
    "    return Model, F_IDS, PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_second_level_model(model_list, class_list, test_df, M_class, boost_settings, settings):\n",
    "    \n",
    "    train_df = pd.read_pickle(settings['DATA_FOLDER'] + 'train_data.csv')\n",
    "    Y = list(train_df['class'])\n",
    "    PREDS = []\n",
    "    for i, class_num in enumerate(class_list):\n",
    "        #Get model\n",
    "        Model = model_list[i]\n",
    "        \n",
    "        #Get lambda-function for dataframe label change\n",
    "        DATAFRAME_LAMBDA = lambda a: 1 if a == class_num else 0\n",
    "        \n",
    "        #Create dataloder instance\n",
    "        train_dataset = CropDataset(train_df, bands=settings['BANDS'], crop_size=settings['INPUT_SIZE'], classf=DATAFRAME_LAMBDA)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=settings['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "        \n",
    "        #Predict\n",
    "        N_PREDS = []\n",
    "        for X, _ in iter(train_dataloader):\n",
    "            N_PREDS.extend(list(Model(X.to(settings['DEVICE'])).squeeze(1).detach().cpu().numpy()))\n",
    "        PREDS.append(N_PREDS)\n",
    "    \n",
    "    #Assemble new train dataframe\n",
    "    #print(PREDS)\n",
    "    colnames = ['class', *[f'feature_{i}' for i in range(len(model_list))]]\n",
    "    train_dataframe = pd.DataFrame(list(zip(Y, *PREDS)), columns=colnames)\n",
    "    \n",
    "    #Clean ram\n",
    "    del PREDS\n",
    "    del train_dataset\n",
    "    del train_dataloader\n",
    "    \n",
    "    #Train 2nd level boosting model\n",
    "    gbm = M_class(**boost_settings)\n",
    "    gbm.fit(train_dataframe.loc[:, 'feature_0':].values, train_dataframe.loc[:, 'class'].values)\n",
    "    \n",
    "    #Predict test_df\n",
    "    PREDS = gbm.predict_proba(test_df.loc[:,'feature_0':])\n",
    "    \n",
    "    #Assemble final test_df\n",
    "    colnames = ['Field_ID', *[F'Crop_ID_{i + 1}' for i in range(7)]]\n",
    "    final_df = pd.DataFrame(list(zip(list(test_df.loc[:, 'field_id']), *PREDS.T.tolist())), columns=colnames)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\archy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\archy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\archy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\archy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\archy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\archy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Users\\archy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#Train 1st lvl models\n",
    "m_list = []\n",
    "predictions = []\n",
    "class_list = [i for i in range(1, 8)]\n",
    "\n",
    "for i in class_list:\n",
    "    Model, F_IDS, preds = train_model(i, **settings)\n",
    "    m_list.append(Model)\n",
    "    predictions.append(preds)\n",
    "\n",
    "colnames = ['field_id', *[f'feature_{i}' for i in range(len(m_list))]]\n",
    "test_dataframe = pd.DataFrame(list(zip(F_IDS, *predictions)), columns=colnames)\n",
    "\n",
    "#Clean some space in ram\n",
    "del F_IDS\n",
    "del predictions\n",
    "\n",
    "#Train 2nd lvl model and get df with predictions\n",
    "final_df = train_second_level_model(m_list, class_list, test_dataframe, model_class, boost_settings, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = final_df.groupby('Field_ID').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crop_ID_1</th>\n",
       "      <th>Crop_ID_2</th>\n",
       "      <th>Crop_ID_3</th>\n",
       "      <th>Crop_ID_4</th>\n",
       "      <th>Crop_ID_5</th>\n",
       "      <th>Crop_ID_6</th>\n",
       "      <th>Crop_ID_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>0.496463</td>\n",
       "      <td>0.201916</td>\n",
       "      <td>0.032132</td>\n",
       "      <td>0.160540</td>\n",
       "      <td>0.052212</td>\n",
       "      <td>0.045939</td>\n",
       "      <td>0.010798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.509048</td>\n",
       "      <td>0.209402</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.131571</td>\n",
       "      <td>0.052153</td>\n",
       "      <td>0.047022</td>\n",
       "      <td>0.018820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>0.545616</td>\n",
       "      <td>0.167482</td>\n",
       "      <td>0.026375</td>\n",
       "      <td>0.159058</td>\n",
       "      <td>0.041659</td>\n",
       "      <td>0.035196</td>\n",
       "      <td>0.024615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1121</td>\n",
       "      <td>0.186545</td>\n",
       "      <td>0.172855</td>\n",
       "      <td>0.015059</td>\n",
       "      <td>0.207878</td>\n",
       "      <td>0.184777</td>\n",
       "      <td>0.180249</td>\n",
       "      <td>0.052637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.508940</td>\n",
       "      <td>0.211539</td>\n",
       "      <td>0.024682</td>\n",
       "      <td>0.131717</td>\n",
       "      <td>0.067764</td>\n",
       "      <td>0.033548</td>\n",
       "      <td>0.021810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4785</td>\n",
       "      <td>0.124357</td>\n",
       "      <td>0.162224</td>\n",
       "      <td>0.137466</td>\n",
       "      <td>0.170029</td>\n",
       "      <td>0.109889</td>\n",
       "      <td>0.179915</td>\n",
       "      <td>0.116121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4788</td>\n",
       "      <td>0.071099</td>\n",
       "      <td>0.037915</td>\n",
       "      <td>0.149457</td>\n",
       "      <td>0.267076</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>0.128107</td>\n",
       "      <td>0.338438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4790</td>\n",
       "      <td>0.142248</td>\n",
       "      <td>0.154683</td>\n",
       "      <td>0.042405</td>\n",
       "      <td>0.189019</td>\n",
       "      <td>0.060790</td>\n",
       "      <td>0.165834</td>\n",
       "      <td>0.245021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4793</td>\n",
       "      <td>0.135378</td>\n",
       "      <td>0.188480</td>\n",
       "      <td>0.178935</td>\n",
       "      <td>0.126478</td>\n",
       "      <td>0.039746</td>\n",
       "      <td>0.169319</td>\n",
       "      <td>0.161665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4794</td>\n",
       "      <td>0.235508</td>\n",
       "      <td>0.022851</td>\n",
       "      <td>0.083602</td>\n",
       "      <td>0.029214</td>\n",
       "      <td>0.109099</td>\n",
       "      <td>0.363437</td>\n",
       "      <td>0.156289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1402 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Crop_ID_1  Crop_ID_2  Crop_ID_3  Crop_ID_4  Crop_ID_5  Crop_ID_6  \\\n",
       "Field_ID                                                                     \n",
       "807        0.496463   0.201916   0.032132   0.160540   0.052212   0.045939   \n",
       "1070       0.509048   0.209402   0.031984   0.131571   0.052153   0.047022   \n",
       "1105       0.545616   0.167482   0.026375   0.159058   0.041659   0.035196   \n",
       "1121       0.186545   0.172855   0.015059   0.207878   0.184777   0.180249   \n",
       "1260       0.508940   0.211539   0.024682   0.131717   0.067764   0.033548   \n",
       "...             ...        ...        ...        ...        ...        ...   \n",
       "4785       0.124357   0.162224   0.137466   0.170029   0.109889   0.179915   \n",
       "4788       0.071099   0.037915   0.149457   0.267076   0.007908   0.128107   \n",
       "4790       0.142248   0.154683   0.042405   0.189019   0.060790   0.165834   \n",
       "4793       0.135378   0.188480   0.178935   0.126478   0.039746   0.169319   \n",
       "4794       0.235508   0.022851   0.083602   0.029214   0.109099   0.363437   \n",
       "\n",
       "          Crop_ID_7  \n",
       "Field_ID             \n",
       "807        0.010798  \n",
       "1070       0.018820  \n",
       "1105       0.024615  \n",
       "1121       0.052637  \n",
       "1260       0.021810  \n",
       "...             ...  \n",
       "4785       0.116121  \n",
       "4788       0.338438  \n",
       "4790       0.245021  \n",
       "4793       0.161665  \n",
       "4794       0.156289  \n",
       "\n",
       "[1402 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check lack of Field_ID:\n",
    "sub_fids = pd.read_csv('SampleSubmission.csv')['Field_ID']\n",
    "out_fids = sub_fids[~sub_fids.isin(sub_df.index)].values\n",
    "\n",
    "#For all lacked ids create random rows (I hope that nothing will went wrong and it will not be necessary)\n",
    "for idx in out_fids:\n",
    "    randrow = np.random.random((1, 7))\n",
    "    randrow = randrow / np.sum(randrow)\n",
    "\n",
    "    sub_df.loc[idx] = randrow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('final-submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
